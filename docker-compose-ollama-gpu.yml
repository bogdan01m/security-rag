services:

  # ollama:
  #   container_name: ollama
  #   pull_policy: always
  #   tty: true
  #   build:
  #     # context: ./services/sec_rag/ollama
  #     dockerfile: ./services/sec_rag/ollama/Dockerfile
  #   env_file:
  #     - .env      
  #   volumes:
  #     - ollama:/root/.ollama
  #   ports:
  #     - "${OLLAMA_PORT}:11434"
  #   restart: always 
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]

  chroma:
    build:
      context: ./services/sec_rag/chroma
    container_name: chroma
    env_file:
      - .env
    environment:
      - CHROMA_PERSIST_DIRECTORY=${CHROMA_PERSIST_DIRECTORY}
      - OLLAMA_HOST=${OLLAMA_HOST}
      - OLLAMA_PORT=${OLLAMA_PORT}
    ports:
      - "${CHROMA_PORT}:8000"  
    volumes:
      - chroma:/app/chroma_db 
    network_mode: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
      

  # sec_rag:
  #   build:
  #     context: ./services/sec_rag
  #   container_name: sec_rag_api
  #   env_file:
  #     - .env
  #   environment:
  #     - MISTRAL_API_KEY=${MISTRAL_API_KEY}
  #     - CHROMA_HOST=${CHROMA_HOST}
  #     - CHROMA_PORT=${CHROMA_PORT}
  #     - OLLAMA_HOST=${OLLAMA_HOST}
  #     - OLLAMA_PORT=$(OLLAMA_PORT)
  #   ports:
  #     - "${LLM_PORT}:8001"  
  #   depends_on:
  #     # - ollama
  #     - chroma
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]




  # base_llm:
  #   build:
  #     context: .
  #     dockerfile: services/base_llm/Dockerfile  
  #   container_name: base_llm_api
  #   env_file:
  #     - .env
  #   ports:
  #     - "${BASE_LLM_PORT}:8002"
  #   environment:
  #     - OLLAMA_HOST=${OLLAMA_HOST}
  #     - OLLAMA_PORT=${OLLAMA_PORT}
  #   depends_on:
  #     - ollama
      # deploy:
      # resources:
      #   reservations:
      #     devices:
      #       - driver: nvidia
      #         count: 1
      #         capabilities: [gpu]


  # telegram_bot:
  #   build:
  #     context: .
  #     dockerfile: services/tg_bot/Dockerfile  
  #   container_name: telegram_bot
  #   env_file:
  #     - .env  
  #   environment:
  #     - BOT_TOKEN=${BOT_TOKEN}  
  #   restart: always  
  #   volumes:
  #     - ./services/tg_bot:/app  


volumes:
  # ollama:
  chroma:


# networks:
#   host:
#     external: true