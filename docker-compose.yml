services:
  chroma:
    build:
      context: ./services/chroma
    container_name: chroma
    env_file:
      - .env
    environment:
      - CHROMA_PERSIST_DIRECTORY=${CHROMA_PERSIST_DIRECTORY}
      - CSV_FILE_PATH=${CSV_FILE_PATH}
      - NVIDIA_VISIBLE_DEVICES=0  # GPU Index
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    ports:
      - "${CHROMA_PORT}:8000"
    volumes:
      - chroma_data:/app/llm_service/chroma_vector_store
      - ./services/chroma:/app
    
    runtime: nvidia

      

  sec_rag:
    build:
      context: .
      dockerfile: services/sec_rag/Dockerfile  
    container_name: sec_rag_api
    env_file:
      - .env
    ports:
      - "${LLM_PORT}:8000"
    environment:
      - MISTRAL_API_KEY=${MISTRAL_API_KEY}
      - CHROMA_HOST=${CHROMA_HOST}
      - CHROMA_PORT=${CHROMA_PORT}
      - NVIDIA_VISIBLE_DEVICES=0  # GPU Index
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    depends_on:
      - chroma
    runtime: nvidia




  base_llm:
    build:
      context: .
      dockerfile: services/base_llm/Dockerfile  
    container_name: base_llm_api
    env_file:
      - .env
    ports:
      - "${BASE_LLM_PORT}:8002"
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST}
      - OLLAMA_PORT=${OLLAMA_PORT}
    depends_on:
      - ollama



  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    env_file:
      - .env
    environment:
      - OLLAMA_DATA=/root/.ollama
      - NVIDIA_VISIBLE_DEVICES=0  # GPU Index
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "${OLLAMA_PORT}:11435"
    runtime: nvidia

  telegram_bot:
    build:
      context: .
      dockerfile: services/tg_bot/Dockerfile  
    container_name: telegram_bot
    env_file:
      - .env  
    environment:
      - BOT_TOKEN=${BOT_TOKEN}  
    restart: always  
    volumes:
      - ./services/tg_bot:/app  


volumes:
  chroma_data:
  ollama_data:

networks:
  sec_rag_network:
    driver: bridge
